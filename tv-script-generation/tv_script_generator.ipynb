{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV script generator\n",
    "\n",
    "Generate TV script from the Simpsons.\n",
    "\n",
    "This is longer than the original as I've added intermediate steps. Helper functions are incorporated here rather than being in a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\\nBart_Simpson: Eh, yeah, hello\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = './data/simpsons/moes_tavern_lines.txt'\n",
    "\n",
    "with open(data_file) as inf:\n",
    "     text= inf.read()\n",
    "\n",
    "# Ignore the \"notice\" section\n",
    "text = text[81:]\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore different parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_sentence_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 10347\n",
      "Total words: 48974\n",
      "Common words: [('the', 1276), ('i', 1242), ('moe_szyslak:', 1180), ('you', 1069), ('a', 1043), ('homer_simpson:', 975), ('to', 847), ('and', 606), ('of', 480), ('my', 467)] ...\n"
     ]
    }
   ],
   "source": [
    "# original:\n",
    "#num_unique_words = len({word: None for word in text.split()})\n",
    "#print('Roughly the number of unique words: {}'.format()\n",
    "\n",
    "## Here we use a counter to find the size of the vocab\n",
    "\n",
    "wc = Counter(text.lower().split())\n",
    "\n",
    "print('Vocab:', len(wc))\n",
    "print('Total words:', sum(wc.values()))\n",
    "print('Common words:', wc.most_common(10), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scenes: 262\n"
     ]
    }
   ],
   "source": [
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sentences in each scene: 15.248091603053435\n"
     ]
    }
   ],
   "source": [
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 4257\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in each line: 11.50434578341555\n"
     ]
    }
   ],
   "source": [
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 0 to 10:\n",
      "\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Sentence {} to {}:'.format(*view_sentence_range))\n",
    "print()\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert vocab to integer representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vocab_to_int and int_to_vocab.\n",
    "# This also sorts from most frequent to least frequent.\n",
    "# For example, integer 0 represents the most frequently used vocab.\n",
    "\n",
    "vocab_to_int = {}\n",
    "int_to_vocab = {}\n",
    "\n",
    "for i, (word, cnt) in enumerate(wc.most_common()):\n",
    "    vocab_to_int[word] = i\n",
    "    int_to_vocab[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'), (1, 'i'), (2, 'moe_szyslak:'), (3, 'you'), (4, 'a')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, int_to_vocab[k]) for k in list(int_to_vocab.keys())[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0), ('i', 1), ('moe_szyslak:', 2), ('you', 3), ('a', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, vocab_to_int[k]) for k in list(vocab_to_int.keys())[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make it into a function suitable for the original project\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    wc = Counter(text)\n",
    "    \n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    \n",
    "    for i, (word, cnt) in enumerate(wc.most_common()):\n",
    "        vocab_to_int[word] = i\n",
    "        int_to_vocab[i] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## quick test\n",
    "\n",
    "text = \"today is an interesting day, is it not?\"\n",
    "text_tokens = text.lower().split()\n",
    "v2i, i2v = create_lookup_tables(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 2, 'day,': 4, 'interesting': 3, 'is': 0, 'it': 5, 'not?': 6, 'today': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'is', 1: 'today', 2: 'an', 3: 'interesting', 4: 'day,', 5: 'it', 6: 'not?'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "## test with given unit test\n",
    "\n",
    "import problem_unittests as tests\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize punctuations\n",
    "\n",
    "We need to distinguish between \"bye\" and \"bye!\".\n",
    "\n",
    "So we will translate punctuations to tokens. For example, change \"!\" into \"||Exclamation_Mark||\". \n",
    "\n",
    "* Period ( . )\n",
    "* Comma ( , )\n",
    "* Quotation Mark ( \" )\n",
    "* Semicolon ( ; )\n",
    "* Exclamation mark ( ! )\n",
    "* Question mark ( ? )\n",
    "* Left Parentheses ( ( )\n",
    "* Right Parentheses ( ) )\n",
    "* Dash ( -- )\n",
    "* Return ( \\n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the string package has a pre-defined list of punctuations\n",
    "\n",
    "import string\n",
    "type(string.punctuation), string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a dictionary according to project\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punct_to_token = {'.':'||PERIOD||', \n",
    "                      ',':'||COMMA||', \n",
    "                      '\"':'||QUOTE||', \n",
    "                      ';':'||SEMICOLON||', \n",
    "                      '!':'||EXCMARK||', \n",
    "                      '?':'||Q||', \n",
    "                      '(':'||OPENP||', \n",
    "                      ')':'||CLOSEP||', \n",
    "                      '--':'||DASH||',\n",
    "                      '\\n':'||NEWLINE||'}\n",
    "    return punct_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': '||NEWLINE||',\n",
       " '!': '||EXCMARK||',\n",
       " '\"': '||QUOTE||',\n",
       " '(': '||OPENP||',\n",
       " ')': '||CLOSEP||',\n",
       " ',': '||COMMA||',\n",
       " '--': '||DASH||',\n",
       " '.': '||PERIOD||',\n",
       " ';': '||SEMICOLON||',\n",
       " '?': '||Q||'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "token_dict = token_lookup()\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "## test with built-in test\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "with open('preprocess.p', 'wb') as outf:\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocess.p',\n",
       " '.ipynb_checkpoints',\n",
       " 'tv_script_generator.ipynb',\n",
       " '__pycache__',\n",
       " 'problem_unittests.py',\n",
       " 'data']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check current directory\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "Simply reload saved files from this point forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9, 9, 10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('preprocess.p', mode='rb') as inf:\n",
    "    int_text, vocab_to_int, int_to_vocab, token_dict = \\\n",
    "        pickle.load(inf)\n",
    "        \n",
    "len(int_text), len(vocab_to_int), len(int_to_vocab), len(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

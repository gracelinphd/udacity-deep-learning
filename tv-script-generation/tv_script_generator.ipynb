{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV script generator\n",
    "\n",
    "Generate TV script from the Simpsons.\n",
    "\n",
    "This is longer than the original as I've added intermediate steps. Helper functions are incorporated here rather than being in a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305189"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = './data/simpsons/moes_tavern_lines.txt'\n",
    "\n",
    "with open(data_file) as inf:\n",
    "     text= inf.read()\n",
    "\n",
    "# Ignore the \"notice\" section\n",
    "text = text[81:]\n",
    "print(text[:100])\n",
    "len(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore different parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_sentence_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 10347\n",
      "Total words: 48974\n",
      "Common words: [('the', 1276), ('i', 1242), ('moe_szyslak:', 1180), ('you', 1069), ('a', 1043), ('homer_simpson:', 975), ('to', 847), ('and', 606), ('of', 480), ('my', 467)] ...\n"
     ]
    }
   ],
   "source": [
    "# original:\n",
    "#num_unique_words = len({word: None for word in text.split()})\n",
    "#print('Roughly the number of unique words: {}'.format()\n",
    "\n",
    "## Here we use a counter to find the size of the vocab\n",
    "\n",
    "wc = Counter(text.lower().split())\n",
    "\n",
    "print('Vocab:', len(wc))\n",
    "print('Total words:', sum(wc.values()))\n",
    "print('Common words:', wc.most_common(10), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scenes: 262\n"
     ]
    }
   ],
   "source": [
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sentences in each scene: 15.248091603053435\n"
     ]
    }
   ],
   "source": [
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 4257\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in each line: 11.50434578341555\n"
     ]
    }
   ],
   "source": [
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 0 to 10:\n",
      "\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Sentence {} to {}:'.format(*view_sentence_range))\n",
    "print()\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert vocab to integer representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vocab_to_int and int_to_vocab.\n",
    "# This also sorts from most frequent to least frequent.\n",
    "# For example, integer 0 represents the most frequently used vocab.\n",
    "\n",
    "vocab_to_int = {}\n",
    "int_to_vocab = {}\n",
    "\n",
    "for i, (word, cnt) in enumerate(wc.most_common()):\n",
    "    vocab_to_int[word] = i\n",
    "    int_to_vocab[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'), (1, 'i'), (2, 'moe_szyslak:'), (3, 'you'), (4, 'a')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, int_to_vocab[k]) for k in list(int_to_vocab.keys())[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0), ('i', 1), ('moe_szyslak:', 2), ('you', 3), ('a', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, vocab_to_int[k]) for k in list(vocab_to_int.keys())[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make it into a function suitable for the original project\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    wc = Counter(text)\n",
    "    \n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    \n",
    "    for i, (word, cnt) in enumerate(wc.most_common()):\n",
    "        vocab_to_int[word] = i\n",
    "        int_to_vocab[i] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## quick test\n",
    "\n",
    "text2 = \"today is an interesting day, is it not?\"\n",
    "text_tokens = text2.lower().split()\n",
    "v2i, i2v = create_lookup_tables(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 2, 'day,': 4, 'interesting': 3, 'is': 0, 'it': 5, 'not?': 6, 'today': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'is', 1: 'today', 2: 'an', 3: 'interesting', 4: 'day,', 5: 'it', 6: 'not?'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "## test with given unit test\n",
    "\n",
    "import problem_unittests as tests\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize punctuations\n",
    "\n",
    "We need to distinguish between \"bye\" and \"bye!\".\n",
    "\n",
    "So we will translate punctuations to tokens. For example, change \"!\" into \"||Exclamation_Mark||\". \n",
    "\n",
    "* Period ( . )\n",
    "* Comma ( , )\n",
    "* Quotation Mark ( \" )\n",
    "* Semicolon ( ; )\n",
    "* Exclamation mark ( ! )\n",
    "* Question mark ( ? )\n",
    "* Left Parentheses ( ( )\n",
    "* Right Parentheses ( ) )\n",
    "* Dash ( -- )\n",
    "* Return ( \\n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the string package has a pre-defined list of punctuations\n",
    "\n",
    "import string\n",
    "type(string.punctuation), string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a dictionary according to project\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punct_to_token = {'.':'||PERIOD||', \n",
    "                      ',':'||COMMA||', \n",
    "                      '\"':'||QUOTE||', \n",
    "                      ';':'||SEMICOLON||', \n",
    "                      '!':'||EXCMARK||', \n",
    "                      '?':'||Q||', \n",
    "                      '(':'||OPENP||', \n",
    "                      ')':'||CLOSEP||', \n",
    "                      '--':'||DASH||',\n",
    "                      '\\n':'||NEWLINE||'}\n",
    "    return punct_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': '||NEWLINE||',\n",
       " '!': '||EXCMARK||',\n",
       " '\"': '||QUOTE||',\n",
       " '(': '||OPENP||',\n",
       " ')': '||CLOSEP||',\n",
       " ',': '||COMMA||',\n",
       " '--': '||DASH||',\n",
       " '.': '||PERIOD||',\n",
       " ';': '||SEMICOLON||',\n",
       " '?': '||Q||'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "token_dict = token_lookup()\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "## test with built-in test\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "with open('preprocess.p', 'wb') as outf:\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak: ||openp|| into phone ||closep|| moe's tavern ||period|| where the elite meet to drink ||period|| "
     ]
    }
   ],
   "source": [
    "_ = [print(int_to_vocab[item], end=\" \") for item in int_text[:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_simpson: eh ||comma|| yeah ||comma|| hello ||comma|| is mike there ||q|| last name ||comma|| rotch ||period|| "
     ]
    }
   ],
   "source": [
    "_ = [print(int_to_vocab[item], end=\" \") for item in int_text[16:32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocess.p',\n",
       " '.ipynb_checkpoints',\n",
       " 'tv_script_generator.ipynb',\n",
       " '.gitignore',\n",
       " 'checkpoint',\n",
       " 'save.index',\n",
       " 'save.data-00000-of-00001',\n",
       " 'save.meta',\n",
       " '__pycache__',\n",
       " 'problem_unittests.py',\n",
       " 'params.p',\n",
       " 'data']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check current directory\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "Simply reload saved files from this point forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69100, 6779, 6779, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('preprocess.p', mode='rb') as inf:\n",
    "    int_text, vocab_to_int, int_to_vocab, token_dict = \\\n",
    "        pickle.load(inf)\n",
    "        \n",
    "len(int_text), len(vocab_to_int), len(int_to_vocab), len(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Input\n",
    "Create placeholder for inputs:\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter.\n",
    "- Targets placeholder.\n",
    "- Learning Rate placeholder.\n",
    "\n",
    "Return the placeholders in the following the tuple (Input, Targets, LearingRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    input = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return (input, targets, learning_rate)\n",
    "\n",
    "## supplied tests\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more BasicLSTMCells in a MultiRNNCell\n",
    "\n",
    "The Rnn size should be set using rnn_size\n",
    "- Initalize Cell State using the MultiRNNCell's zero_state() function\n",
    "- Apply the name \"initial_state\" to the initial state using tf.identity()\n",
    "- Return the cell and initial state in the following tuple (Cell, InitialState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "\n",
    "    # this doesn't seem to work\n",
    "    #lstm_layer = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([lstm_layer] * num_layers)\n",
    "    \n",
    "    # use staked version instead:\n",
    "    stacked_rnn = []\n",
    "    for i in range(num_layers):\n",
    "        stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(num_units=512, state_is_tuple=True))\n",
    "\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
    "    \n",
    "    # yet another version\n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([lstm_layer, lstm_layer])\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    return (cell, initial_state)\n",
    "\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply word embedding to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    #embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    #embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    embed = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                            vocab_size=vocab_size, \n",
    "                                            embed_dim=embed_dim)\n",
    "    return embed\n",
    "\n",
    "\n",
    "## supplied test\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the get_init_cell() function. Time to use the cell to create a RNN.\n",
    "- Build the RNN using the tf.nn.dynamic_rnn()\n",
    "- Apply the name \"final_state\" to the final state using tf.identity()\n",
    "- Return the outputs and final_state state in the following tuple (Outputs, FinalState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    return outputs, final_state\n",
    "\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to input_data using your get_embed(input_data, vocab_size, embed_dim) function.\n",
    "- Build RNN using cell and your build_rnn(cell, inputs) function.\n",
    "- Apply a fully connected layer with a linear activation and vocab_size as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim=embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    #logits = tf.contrib.layers.fully_connected(outputs, vocab_size)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_get_batches(get_batches):\n",
    "    with tf.Graph().as_default():\n",
    "        test_batch_size = 128\n",
    "        test_seq_length = 5\n",
    "        test_int_text = list(range(1000*test_seq_length))\n",
    "        batches = get_batches(test_int_text, test_batch_size, test_seq_length)\n",
    "\n",
    "        # Check type\n",
    "        assert isinstance(batches, np.ndarray),\\\n",
    "            'Batches is not a Numpy array'\n",
    "\n",
    "        # Check shape\n",
    "        assert batches.shape == (7, 2, 128, 5),\\\n",
    "            'Batches returned wrong shape.  Found {}'.format(batches.shape)\n",
    "\n",
    "    tests._print_success_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement get_batches to create batches of input and targets using int_text. The batches should be a Numpy array with the shape (number of batches, 2, batch size, sequence length). Each batch contains two elements:\n",
    "- The first element is a single batch of input with the shape [batch size, sequence length]\n",
    "- The second element is a single batch of targets with the shape [batch size, sequence length]\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3) would return a Numpy array of the following:\n",
    "\n",
    "[\n",
    "  \\# First Batch <br>\n",
    "  [\n",
    "    \\# Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    \\# Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    "\n",
    "  \\# Second Batch\n",
    "  [\n",
    "    \\# Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    \\# Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    int_text = int_text[16:]\n",
    "    n_batches = (len(int_text)-1)//(batch_size * seq_length)    \n",
    "    int_text = int_text[:n_batches * batch_size * seq_length + 1]\n",
    "    int_text_sequences = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "    int_text = int_text[1:]\n",
    "    int_text_targets = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "    output = []\n",
    "    for batch in range(n_batches):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for size in range(batch_size):\n",
    "            inputs.append(int_text_sequences[size * n_batches + batch])\n",
    "            targets.append(int_text_targets[size * n_batches + batch])\n",
    "        output.append([inputs, targets])\n",
    "    return np.array(output)\n",
    "\n",
    "## check out why this fails\n",
    "#tests.test_get_batches(get_batches)\n",
    "test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### NN Training\n",
    "Tune hyperparameters:\n",
    "- Set num_epochs to the number of epochs.\n",
    "- Set batch_size to the batch size.\n",
    "- Set rnn_size to the size of the RNNs.\n",
    "- Set seq_length to the length of sequence.\n",
    "- Set learning_rate to the learning rate.\n",
    "- Set show_every_n_batches to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "batch_size = 101\n",
    "rnn_size = 256\n",
    "seq_length = 20\n",
    "learning_rate = 0.01\n",
    "embed_dim = 300\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "#show_every_n_batches= get_batches(int_text, batch_size, seq_length).shape[0]\n",
    "print(show_every_n_batches)\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 69084\n",
      "Batch Size -> words missed\n",
      "\n",
      "        96    1884 \n",
      "        97    1184 \n",
      "        98     484 \n",
      "        99    1764 \n",
      "       100    1084 \n",
      "       101     404 <<< Current choice. Should be minimum words missed.\n",
      "       102    1764 \n",
      "       103    1104 \n",
      "       104     444 \n",
      "       105    1884 \n"
     ]
    }
   ],
   "source": [
    "## determine batch size (from someone else)\n",
    "\n",
    "total_words = len(int_text[16:])\n",
    "print('total words:', total_words)\n",
    "\n",
    "print(\"Batch Size -> words missed\\n\")\n",
    "\n",
    "for i in range(-5,5):\n",
    "    try_batch_size = batch_size + i\n",
    "    batches = get_batches(int_text, try_batch_size, seq_length)\n",
    "    flag = \"\"\n",
    "    if i == 0:\n",
    "        flag = \"<<< Current choice. Should be minimum words missed.\"\n",
    "    print(\"{:>10}   {:>5} {}\".format(try_batch_size, total_words - try_batch_size * seq_length * batches.shape[0], flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/34   train_loss = 8.822\n",
      "Epoch   2 Batch   32/34   train_loss = 5.478\n",
      "Epoch   5 Batch   30/34   train_loss = 4.681\n",
      "Epoch   8 Batch   28/34   train_loss = 4.521\n",
      "Epoch  11 Batch   26/34   train_loss = 4.369\n",
      "Epoch  14 Batch   24/34   train_loss = 3.921\n",
      "Epoch  17 Batch   22/34   train_loss = 3.596\n",
      "Epoch  20 Batch   20/34   train_loss = 3.482\n",
      "Epoch  23 Batch   18/34   train_loss = 3.098\n",
      "Epoch  26 Batch   16/34   train_loss = 2.767\n",
      "Epoch  29 Batch   14/34   train_loss = 2.693\n",
      "Epoch  32 Batch   12/34   train_loss = 2.344\n",
      "Epoch  35 Batch   10/34   train_loss = 2.117\n",
      "Epoch  38 Batch    8/34   train_loss = 1.958\n",
      "Epoch  41 Batch    6/34   train_loss = 1.691\n",
      "Epoch  44 Batch    4/34   train_loss = 1.585\n",
      "Epoch  47 Batch    2/34   train_loss = 1.749\n",
      "Epoch  50 Batch    0/34   train_loss = 1.591\n",
      "Epoch  52 Batch   32/34   train_loss = 1.317\n",
      "Epoch  55 Batch   30/34   train_loss = 0.979\n",
      "Epoch  58 Batch   28/34   train_loss = 0.769\n",
      "Epoch  61 Batch   26/34   train_loss = 0.671\n",
      "Epoch  64 Batch   24/34   train_loss = 0.572\n",
      "Epoch  67 Batch   22/34   train_loss = 0.555\n",
      "Epoch  70 Batch   20/34   train_loss = 0.538\n",
      "Epoch  73 Batch   18/34   train_loss = 0.458\n",
      "Epoch  76 Batch   16/34   train_loss = 0.448\n",
      "Epoch  79 Batch   14/34   train_loss = 0.423\n",
      "Epoch  82 Batch   12/34   train_loss = 0.388\n",
      "Epoch  85 Batch   10/34   train_loss = 0.421\n",
      "Epoch  88 Batch    8/34   train_loss = 0.720\n",
      "Epoch  91 Batch    6/34   train_loss = 1.057\n",
      "Epoch  94 Batch    4/34   train_loss = 1.211\n",
      "Epoch  97 Batch    2/34   train_loss = 1.013\n",
      "Epoch 100 Batch    0/34   train_loss = 0.632\n",
      "Epoch 102 Batch   32/34   train_loss = 0.419\n",
      "Epoch 105 Batch   30/34   train_loss = 0.288\n",
      "Epoch 108 Batch   28/34   train_loss = 0.212\n",
      "Epoch 111 Batch   26/34   train_loss = 0.201\n",
      "Epoch 114 Batch   24/34   train_loss = 0.191\n",
      "Epoch 117 Batch   22/34   train_loss = 0.174\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "\n",
    "Save seq_length and save_dir for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "params = (seq_length, save_dir)\n",
    "pickle.dump(params, open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Generate Functions\n",
    "\n",
    "Get tensors from loaded_graph using the function get_tensor_by_name(). Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\") \n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\") \n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    return  (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Select the next word using probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(choices):\n",
    "    # From http://stackoverflow.com/questions/3679694/a-weighted-version-of-random-choice\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    assert False, \"Shouldn't get here\"\n",
    "    \n",
    "def pick_from_top_5(choices):\n",
    "    top5 = []\n",
    "    for i in range(min(len(choices), 5)):\n",
    "        index = np.argmax(choices)\n",
    "        top5.append((index, choices[index]))\n",
    "        choices.itemset(index, 0) # Avoid picking this index as argmax again\n",
    "    return weighted_choice(top5)\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return int_to_vocab[pick_from_top_5(probabilities)]\n",
    "\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TV Script\n",
    "\n",
    "Set gen_length to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "homer_simpson:(loud whisper) let's try what chapter seven calls\" un-sults\"-- insults disguised as compliments.(explaining) from the point-- and that i don't just have to get you to be a shill, he's how.\n",
      "moe_szyslak: yeah, yeah, but you'd feel bad inside.\n",
      "lenny_leonard: plus they'll say, they took off the corner.\n",
      "\n",
      "\n",
      "moe_szyslak: homer, no, i can really do something.\n",
      "moe_szyslak: sorry, homer. you know what's that school--\n",
      "lenny_leonard: well, i run your\n"
     ]
    }
   ],
   "source": [
    "gen_length = 100\n",
    "\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'homer_simpson'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        probabilities = probabilities[0]\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
